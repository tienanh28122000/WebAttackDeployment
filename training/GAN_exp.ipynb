{"cells":[{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import math\n","import os\n","import pandas as pd\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:54:33.825153Z","iopub.status.busy":"2023-04-18T15:54:33.824719Z","iopub.status.idle":"2023-04-18T15:54:33.835480Z","shell.execute_reply":"2023-04-18T15:54:33.833966Z","shell.execute_reply.started":"2023-04-18T15:54:33.825107Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T15:54:33.838924Z","iopub.status.busy":"2023-04-18T15:54:33.838359Z","iopub.status.idle":"2023-04-18T15:54:33.846653Z","shell.execute_reply":"2023-04-18T15:54:33.845387Z","shell.execute_reply.started":"2023-04-18T15:54:33.838877Z"},"trusted":true},"outputs":[],"source":["config = {\n","    \"num_labels\": 7,\n","    \"hidden_dropout_prob\": 0.15,\n","    \"hidden_size\": 768,\n","    \"max_length\": 512,\n","}\n","\n","training_parameters = {\n","    \"batch_size\": 2,\n","    \"epochs\": 1,\n","    \"output_folder\": \"model_weight\",\n","    \"output_file\": \"model.pt\",\n","    \"learning_rate\": 2e-5,\n","    \"print_after_steps\": 100,\n","    \"save_steps\": 500,\n","\n","}"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:22.461237Z","iopub.status.busy":"2023-04-18T16:06:22.460206Z","iopub.status.idle":"2023-04-18T16:06:22.473511Z","shell.execute_reply":"2023-04-18T16:06:22.472389Z","shell.execute_reply.started":"2023-04-18T16:06:22.461200Z"},"trusted":true},"outputs":[],"source":["# from transformers import BertTokenizer, BertModel\n","from transformers import AutoTokenizer, AutoModel\n","class ReviewDataset(Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","        self.tokenizer = AutoTokenizer.from_pretrained('jackaduma/SecBERT')\n","\n","    def __getitem__(self, index):\n","        review = self.df.iloc[index][\"text\"]\n","        sentiment = self.df.iloc[index][\"label\"]\n","        sentiment_dict = {'000 - Normal': 0,\n","          '126 - Path Traversal': 1,\n","          '242 - Code Injection': 2,\n","          '153 - Input Data Manipulation': 3,\n","          '310 - Scanning for Vulnerable Software': 4,\n","          '194 - Fake the Source of Data': 5,\n","          '34 - HTTP Response Splitting': 6}\n","        label = sentiment_dict[sentiment]\n","        encoded_input = self.tokenizer.encode_plus(\n","                review,\n","                add_special_tokens=True,\n","                max_length= config[\"max_length\"],\n","                pad_to_max_length=True,\n","                return_overflowing_tokens=True,\n","            )\n","        if \"num_truncated_tokens\" in encoded_input and encoded_input[\"num_truncated_tokens\"] > 0:\n","            # print(\"Attention! you are cropping tokens\")\n","            pass\n","\n","        input_ids = encoded_input[\"input_ids\"]\n","        attention_mask = encoded_input[\"attention_mask\"] if \"attention_mask\" in encoded_input else None\n","\n","        token_type_ids = encoded_input[\"token_type_ids\"] if \"token_type_ids\" in encoded_input else None\n","\n","\n","\n","        data_input = {\n","            \"input_ids\": torch.tensor(input_ids),\n","            \"attention_mask\": torch.tensor(attention_mask),\n","            \"token_type_ids\": torch.tensor(token_type_ids),\n","            \"label\": torch.tensor(label),\n","        }\n","\n","        return data_input[\"input_ids\"], data_input[\"attention_mask\"], data_input[\"token_type_ids\"], data_input[\"label\"]\n","\n","\n","\n","    def __len__(self):\n","        return self.df.shape[0]"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:26.366661Z","iopub.status.busy":"2023-04-18T16:06:26.365992Z","iopub.status.idle":"2023-04-18T16:06:27.092184Z","shell.execute_reply":"2023-04-18T16:06:27.091036Z","shell.execute_reply.started":"2023-04-18T16:06:26.366623Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>GET /blog/index.php/2020/04/04/voluptatum-repr...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GET /blog/xmlrpc.php?rsd</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GET /blog/index.php/2020/04/04/nihil-tenetur-e...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GET /blog/index.php/2020/04/04/explicabo-qui-f...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>GET /blog/index.php/2020/04/04/explicabo-qui-f...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text         label\n","0  GET /blog/index.php/2020/04/04/voluptatum-repr...  000 - Normal\n","1                           GET /blog/xmlrpc.php?rsd  000 - Normal\n","2  GET /blog/index.php/2020/04/04/nihil-tenetur-e...  000 - Normal\n","3  GET /blog/index.php/2020/04/04/explicabo-qui-f...  000 - Normal\n","4  GET /blog/index.php/2020/04/04/explicabo-qui-f...  000 - Normal"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":["df_train = pd.read_csv('../dataset/dataset_capec_combine.csv')\n","df_train.head()"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:28.176868Z","iopub.status.busy":"2023-04-18T16:06:28.176170Z","iopub.status.idle":"2023-04-18T16:06:28.553311Z","shell.execute_reply":"2023-04-18T16:06:28.552252Z","shell.execute_reply.started":"2023-04-18T16:06:28.176822Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>GET  blog index.php 2020 04 04 voluptatum-repr...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>GET  blog xmlrpc.php?rsd</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GET  blog index.php 2020 04 04 nihil-tenetur-e...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GET  blog index.php 2020 04 04 explicabo-qui-f...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>GET  blog index.php 2020 04 04 explicabo-qui-f...</td>\n","      <td>000 - Normal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text         label\n","0  GET  blog index.php 2020 04 04 voluptatum-repr...  000 - Normal\n","1                           GET  blog xmlrpc.php?rsd  000 - Normal\n","2  GET  blog index.php 2020 04 04 nihil-tenetur-e...  000 - Normal\n","3  GET  blog index.php 2020 04 04 explicabo-qui-f...  000 - Normal\n","4  GET  blog index.php 2020 04 04 explicabo-qui-f...  000 - Normal"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["# Optional (not effect very much)\n","df_train['text'] = df_train['text'].str.replace('/',' ')\n","df_train.head()"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:29.715467Z","iopub.status.busy":"2023-04-18T16:06:29.715082Z","iopub.status.idle":"2023-04-18T16:06:30.161883Z","shell.execute_reply":"2023-04-18T16:06:30.160838Z","shell.execute_reply.started":"2023-04-18T16:06:29.715434Z"},"trusted":true},"outputs":[],"source":["## Reduce data for testing\n","df_242 = df_train[(df_train['label'] == '242 - Code Injection')]\n","df_242 = df_242.sample(frac = 1)\n","df_242 = df_242[:50000]\n","df_000 = df_train[(df_train['label'] == '000 - Normal')]\n","df_000 = df_000.sample(frac = 1)\n","df_000 = df_000[:50000]\n","\n","df_sub = df_train[(df_train['label'] != '000 - Normal') & (df_train['label'] != '242 - Code Injection')]\n","\n","df_train = pd.concat([df_train,df_242,df_000], ignore_index=True)"]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:31.558735Z","iopub.status.busy":"2023-04-18T16:06:31.558358Z","iopub.status.idle":"2023-04-18T16:06:32.180289Z","shell.execute_reply":"2023-04-18T16:06:32.179270Z","shell.execute_reply.started":"2023-04-18T16:06:31.558701Z"},"trusted":true},"outputs":[],"source":["## prep\n","source_dataset = ReviewDataset(df_train)\n","source_dataloader = DataLoader(dataset = source_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:32.203420Z","iopub.status.busy":"2023-04-18T16:06:32.202770Z","iopub.status.idle":"2023-04-18T16:06:32.238860Z","shell.execute_reply":"2023-04-18T16:06:32.237735Z","shell.execute_reply.started":"2023-04-18T16:06:32.203376Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>POST /vendor/phpunit/phpunit/src/Util/PHP/eval...</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>POST /cgi-bin/ViewLog.asp  remote_submit_Flag=...</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GET /.svn/wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GET /blog/.svn/wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>GET /blog/index.php/my-account/.svn/wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  \\\n","0  POST /vendor/phpunit/phpunit/src/Util/PHP/eval...   \n","1  POST /cgi-bin/ViewLog.asp  remote_submit_Flag=...   \n","2                                    GET /.svn/wc.db   \n","3                               GET /blog/.svn/wc.db   \n","4          GET /blog/index.php/my-account/.svn/wc.db   \n","\n","                           label  \n","0  153 - Input Data Manipulation  \n","1  153 - Input Data Manipulation  \n","2  153 - Input Data Manipulation  \n","3  153 - Input Data Manipulation  \n","4  153 - Input Data Manipulation  "]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["df_transfer = pd.read_csv('../dataset/dataset_capec_transfer.csv')\n","df_transfer.head()"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:32.806661Z","iopub.status.busy":"2023-04-18T16:06:32.804638Z","iopub.status.idle":"2023-04-18T16:06:32.834141Z","shell.execute_reply":"2023-04-18T16:06:32.833041Z","shell.execute_reply.started":"2023-04-18T16:06:32.806608Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>POST  vendor phpunit phpunit src Util PHP eval...</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>POST  cgi-bin ViewLog.asp  remote_submit_Flag=...</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GET  .svn wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GET  blog .svn wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>GET  blog index.php my-account .svn wc.db</td>\n","      <td>153 - Input Data Manipulation</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  \\\n","0  POST  vendor phpunit phpunit src Util PHP eval...   \n","1  POST  cgi-bin ViewLog.asp  remote_submit_Flag=...   \n","2                                    GET  .svn wc.db   \n","3                               GET  blog .svn wc.db   \n","4          GET  blog index.php my-account .svn wc.db   \n","\n","                           label  \n","0  153 - Input Data Manipulation  \n","1  153 - Input Data Manipulation  \n","2  153 - Input Data Manipulation  \n","3  153 - Input Data Manipulation  \n","4  153 - Input Data Manipulation  "]},"execution_count":93,"metadata":{},"output_type":"execute_result"}],"source":["# Optional (not effect very much)\n","df_transfer['text'] = df_transfer['text'].str.replace('/',' ')\n","df_transfer.head()"]},{"cell_type":"code","execution_count":94,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:33.870857Z","iopub.status.busy":"2023-04-18T16:06:33.869966Z","iopub.status.idle":"2023-04-18T16:06:34.382962Z","shell.execute_reply":"2023-04-18T16:06:34.381990Z","shell.execute_reply.started":"2023-04-18T16:06:33.870818Z"},"trusted":true},"outputs":[],"source":["target_dataset = ReviewDataset(df_transfer)\n","target_dataloader = DataLoader(dataset = target_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)"]},{"cell_type":"code","execution_count":95,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:36.193370Z","iopub.status.busy":"2023-04-18T16:06:36.192982Z","iopub.status.idle":"2023-04-18T16:06:36.199966Z","shell.execute_reply":"2023-04-18T16:06:36.198846Z","shell.execute_reply.started":"2023-04-18T16:06:36.193313Z"},"trusted":true},"outputs":[],"source":["from torch.autograd import Function\n","\n","\n","class GradientReversalFn(Function):\n","    @staticmethod\n","    def forward(ctx, x, alpha):\n","        ctx.alpha = alpha\n","        \n","        return x.view_as(x)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        output = grad_output.neg() * ctx.alpha\n","\n","        return output, None"]},{"cell_type":"code","execution_count":96,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:13:38.500954Z","iopub.status.busy":"2023-04-18T16:13:38.500234Z","iopub.status.idle":"2023-04-18T16:13:38.511746Z","shell.execute_reply":"2023-04-18T16:13:38.510600Z","shell.execute_reply.started":"2023-04-18T16:13:38.500916Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class DomainAdaptationModel(nn.Module):\n","    def __init__(self):\n","        super(DomainAdaptationModel, self).__init__()\n","        \n","        num_labels = config[\"num_labels\"]\n","        self.bert = AutoModel.from_pretrained('jackaduma/SecBERT')\n","        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n","        self.sentiment_classifier = nn.Sequential(\n","            nn.Linear(config[\"hidden_size\"], num_labels),\n","            nn.LogSoftmax(dim=1),\n","        )\n","        self.domain_classifier = nn.Sequential(\n","            nn.Linear(config[\"hidden_size\"], 2),\n","            nn.LogSoftmax(dim=1),\n","        )\n","\n","\n","    def forward(\n","          self,\n","          input_ids=None,\n","          attention_mask=None,\n","          token_type_ids=None,\n","          labels=None,\n","          grl_lambda = 1.0, \n","          ):\n","\n","        outputs = self.bert(\n","                input_ids,\n","                attention_mask=attention_mask,\n","                token_type_ids=token_type_ids,\n","            )\n","\n","#         pooled_output = outputs[1] # For bert-base-uncase\n","        pooled_output = outputs.pooler_output \n","        pooled_output = self.dropout(pooled_output)\n","\n","\n","        reversed_pooled_output = GradientReversalFn.apply(pooled_output, grl_lambda)\n","\n","        sentiment_pred = self.sentiment_classifier(pooled_output)\n","        domain_pred = self.domain_classifier(reversed_pooled_output)\n","\n","        return sentiment_pred.to(device), domain_pred.to(device)"]},{"cell_type":"code","execution_count":97,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:06:39.021194Z","iopub.status.busy":"2023-04-18T16:06:39.020102Z","iopub.status.idle":"2023-04-18T16:06:39.028509Z","shell.execute_reply":"2023-04-18T16:06:39.027435Z","shell.execute_reply.started":"2023-04-18T16:06:39.021153Z"},"trusted":true},"outputs":[],"source":["def compute_accuracy(logits, labels):\n","    \n","    predicted_labels_dict = {\n","      0: 0,\n","      1: 0,\n","      2: 0,\n","      3: 0,\n","      4: 0,\n","      5: 0,\n","      6: 0,\n","    }\n","    \n","    predicted_label = logits.max(dim = 1)[1]\n","    \n","    for pred in predicted_label:\n","        # print(pred.item())\n","        predicted_labels_dict[pred.item()] += 1\n","    acc = (predicted_label == labels).float().mean()\n","    \n","    return acc, predicted_labels_dict"]},{"cell_type":"code","execution_count":98,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:52:18.153193Z","iopub.status.busy":"2023-04-18T16:52:18.152791Z","iopub.status.idle":"2023-04-18T16:52:18.167463Z","shell.execute_reply":"2023-04-18T16:52:18.166223Z","shell.execute_reply.started":"2023-04-18T16:52:18.153158Z"},"trusted":true},"outputs":[],"source":["def evaluate(model, dataset = \"transfer\", percentage = 5):\n","    with torch.no_grad():\n","        predicted_labels_dict = {                                                   \n","          0: 0,\n","          1: 0,\n","          2: 0,\n","          3: 0,\n","          4: 0,\n","          5: 0,\n","          6: 0,                                                                   \n","        }\n","        \n","        dev_df = pd.read_csv(\"../dataset/dataset_capec_\" + dataset + \".csv\")\n","        data_size = dev_df.shape[0]\n","        selected_for_evaluation = int(data_size*percentage/100)\n","        dev_df = dev_df.head(selected_for_evaluation)\n","        dataset = ReviewDataset(dev_df)\n","\n","        dataloader = DataLoader(dataset = dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n","\n","        mean_accuracy = 0.0\n","        total_batches = len(dataloader)\n","        \n","        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n","            inputs = {\n","                \"input_ids\": input_ids.squeeze(axis=1),\n","                \"attention_mask\": attention_mask.squeeze(axis=1),\n","                \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n","                \"labels\": labels,\n","            }\n","            for k, v in inputs.items():\n","                inputs[k] = v.to(device)\n","\n","\n","            sentiment_pred, _ = model(**inputs)\n","            predicted_label = sentiment_pred.max(dim = 1)[1]\n","            accuracy, predicted_labels = compute_accuracy(sentiment_pred, inputs[\"labels\"])\n","            mean_accuracy += accuracy\n","            for i in range(7): \n","              predicted_labels_dict[i] += predicted_labels[i]\n","\n","        print(predicted_label)\n","    return mean_accuracy/total_batches"]},{"cell_type":"code","execution_count":99,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:32:19.369124Z","iopub.status.busy":"2023-04-18T16:32:19.368752Z","iopub.status.idle":"2023-04-18T16:47:04.671093Z","shell.execute_reply":"2023-04-18T16:47:04.669299Z","shell.execute_reply.started":"2023-04-18T16:32:19.369091Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/home/anhnmt2/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/home/anhnmt2/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/home/anhnmt2/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/home/anhnmt2/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Training Step: 0\n","Training Step: 100\n","Training Step: 200\n","Training Step: 300\n","Training Step: 400\n","Training Step: 500\n","Training Step: 600\n","Training Step: 700\n","Training Step: 800\n","Training Step: 900\n","Training Step: 1000\n","Training Step: 1100\n","Training Step: 1200\n","Training Step: 1300\n","Training Step: 1400\n","Training Step: 1500\n","Training Step: 1600\n","Training Step: 1700\n","Training Step: 1800\n","Training Step: 1900\n","Training Step: 2000\n","Training Step: 2100\n","Training Step: 2200\n","Training Step: 2300\n","Training Step: 2400\n","Training Step: 2500\n","Training Step: 2600\n","Training Step: 2700\n","Training Step: 2800\n","Training Step: 2900\n","Training Step: 3000\n","Training Step: 3100\n","Training Step: 3200\n","Training Step: 3300\n","Training Step: 3400\n","Training Step: 3500\n","Training Step: 3600\n","Training Step: 3700\n","Training Step: 3800\n","Training Step: 3900\n","Training Step: 4000\n","Training Step: 4100\n","Training Step: 4200\n","Training Step: 4300\n","Training Step: 4400\n","Training Step: 4500\n","Training Step: 4600\n","Training Step: 4700\n","Training Step: 4800\n","Training Step: 4900\n","Training Step: 5000\n","Training Step: 5100\n","Training Step: 5200\n","Training Step: 5300\n","Training Step: 5400\n","Training Step: 5500\n","Training Step: 5600\n","Training Step: 5700\n","Training Step: 5800\n","Training Step: 5900\n"]}],"source":["lr = training_parameters[\"learning_rate\"]\n","n_epochs = training_parameters[\"epochs\"]\n","\n","model = DomainAdaptationModel()\n","model.to(device)\n","\n","optimizer = optim.Adam(model.parameters(), lr)\n","\n","loss_fn_sentiment_classifier = torch.nn.NLLLoss()\n","loss_fn_domain_classifier = torch.nn.NLLLoss()\n","'''\n","In one training step we will update the model using both the source labeled data and target unlabeled data\n","We will run it till the batches last for any of these datasets\n","\n","In our case target dataset has more data. Hence, we will leverage the entire source dataset for training\n","\n","If we use the same approach in a case where the source dataset has more data then the target dataset then we will\n","under-utilize the labeled source dataset. In such a scenario it is better to reload the target dataset when it finishes\n","This will ensure that we are utilizing the entire source dataset to train our model.\n","'''\n","\n","max_batches = min(len(source_dataloader), len(target_dataloader))\n","\n","for epoch_idx in range(n_epochs):\n","    \n","    source_iterator = iter(source_dataloader)\n","    target_iterator = iter(target_dataloader)\n","\n","    for batch_idx in range(max_batches):\n","        \n","        p = float(batch_idx + epoch_idx * max_batches) / (training_parameters[\"epochs\"] * max_batches)\n","        grl_lambda = 2. / (1. + np.exp(-10 * p)) - 1\n","        grl_lambda = torch.tensor(grl_lambda)\n","        \n","        model.train()\n","        \n","        if(batch_idx%training_parameters[\"print_after_steps\"] == 0 ):\n","            print(\"Training Step:\", batch_idx)\n","        \n","        optimizer.zero_grad()\n","        \n","        # Souce dataset training update\n","        input_ids, attention_mask, token_type_ids, labels = next(source_iterator)\n","        inputs = {\n","            \"input_ids\": input_ids.squeeze(axis=1),\n","            \"attention_mask\": attention_mask.squeeze(axis=1),\n","            \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n","            \"labels\" : labels,\n","            \"grl_lambda\" : grl_lambda,\n","        }\n","\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","    \n","        sentiment_pred, domain_pred = model(**inputs)\n","        loss_s_sentiment = loss_fn_sentiment_classifier(sentiment_pred, inputs[\"labels\"])\n","        y_s_domain = torch.zeros(training_parameters[\"batch_size\"], dtype=torch.long).to(device)\n","        loss_s_domain = loss_fn_domain_classifier(domain_pred, y_s_domain)\n","\n","\n","        # Target dataset training update \n","        input_ids, attention_mask, token_type_ids, labels = next(target_iterator)\n","        inputs = {\n","            \"input_ids\": input_ids.squeeze(axis=1),\n","            \"attention_mask\": attention_mask.squeeze(axis=1),\n","            \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n","            \"labels\" : labels,\n","            \"grl_lambda\" : grl_lambda,\n","        }\n","\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","    \n","        _, domain_pred = model(**inputs)\n","        \n","        # Note that we are not using the sentiment predictions here for updating the weights\n","        y_t_domain = torch.ones(input_ids.shape[0], dtype=torch.long).to(device)\n","        # print(domain_pred.shape, y_t_domain.shape)\n","        loss_t_domain = loss_fn_domain_classifier(domain_pred, y_t_domain)\n","\n","        # Combining the loss \n","\n","        loss = loss_s_sentiment + loss_s_domain + loss_t_domain\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluate the model after every epoch\n","    \n","    # torch.save(model.state_dict(), os.path.join(training_parameters[\"output_folder\"], \"epoch_\" + str(epoch_idx)  +  training_parameters[\"output_file\"] ))\n","    torch.save(model, os.path.join(training_parameters[\"output_folder\"], \"epoch_\" + str(epoch_idx)  +  training_parameters[\"output_file\"] ))\n","#     accuracy = evaluate(model, dataset = \"combine\", percentage = 1).item()\n","#     print(\"Accuracy on amazon after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))\n","\n","    # accuracy = evaluate(model, dataset = \"transfer\", percentage = 100).item()\n","    # print(\"Accuracy on transfer dataset after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2023-04-18T16:52:32.160221Z","iopub.status.busy":"2023-04-18T16:52:32.159031Z","iopub.status.idle":"2023-04-18T16:54:51.158166Z","shell.execute_reply":"2023-04-18T16:54:51.157030Z","shell.execute_reply.started":"2023-04-18T16:52:32.160172Z"},"trusted":true},"outputs":[],"source":["# accuracy = evaluate(model, dataset = \"transfer\", percentage = 100).item()\n","# print(\"Accuracy on transfer dataset after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"data":{"text/plain":["DomainAdaptationModel(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(52000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(514, 768)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-5): 6 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.15, inplace=False)\n","  (sentiment_classifier): Sequential(\n","    (0): Linear(in_features=768, out_features=7, bias=True)\n","    (1): LogSoftmax(dim=1)\n","  )\n","  (domain_classifier): Sequential(\n","    (0): Linear(in_features=768, out_features=2, bias=True)\n","    (1): LogSoftmax(dim=1)\n","  )\n",")"]},"execution_count":106,"metadata":{},"output_type":"execute_result"}],"source":["weight_path = \"model_weight/epoch_0model.pt\"\n","\n","model_test = torch.load(weight_path)\n","model_test.eval()"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/home/anhnmt2/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/home/anhnmt2/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["tensor([2], device='cuda:0')\n","Accuracy on transfer dataset is 0.5252960920333862\n"]}],"source":["accuracy = evaluate(model_test, dataset = \"transfer\", percentage = 100).item()\n","print(\"Accuracy on transfer dataset is \" + str(accuracy))"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["def pred(model, dataset = \"transfer\", percentage = 5):\n","    all_pred = []\n","    with torch.no_grad():\n","        \n","        dev_df = pd.read_csv(\"../dataset/dataset_capec_\" + dataset + \".csv\")\n","        data_size = dev_df.shape[0]\n","        selected_for_evaluation = int(data_size*percentage/100)\n","        dev_df = dev_df.head(selected_for_evaluation)\n","        dataset = ReviewDataset(dev_df)\n","\n","        dataloader = DataLoader(dataset = dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n","        \n","        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n","            inputs = {\n","                \"input_ids\": input_ids.squeeze(axis=1),\n","                \"attention_mask\": attention_mask.squeeze(axis=1),\n","                \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n","                \"labels\": labels,\n","            }\n","            for k, v in inputs.items():\n","                inputs[k] = v.to(device)\n","\n","            sentiment_pred, _ = model(**inputs)\n","            pred_label = sentiment_pred.max(dim = 1)[1]\n","            for pred in pred_label:\n","                all_pred.append(pred.item())\n","    return all_pred"]},{"cell_type":"code","execution_count":109,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/home/anhnmt2/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/home/anhnmt2/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["pred_list = pred(model_test, dataset = \"transfer\", percentage = 10)"]},{"cell_type":"code","execution_count":110,"metadata":{},"outputs":[{"data":{"text/plain":["[1,\n"," 0,\n"," 0,\n"," 2,\n"," 0,\n"," 1,\n"," 2,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 2,\n"," 1,\n"," 0,\n"," 1,\n"," 4,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 2,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 4,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 2,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 2,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 2,\n"," 1,\n"," 1,\n"," 1,\n"," 4,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 2,\n"," 0,\n"," 1,\n"," 0,\n"," 2,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 2,\n"," 1,\n"," 2,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 2,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 2,\n"," 2,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 4,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 4,\n"," 1,\n"," 0,\n"," 2,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 2,\n"," 4,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 2,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 4,\n"," 0,\n"," 0,\n"," 0,\n"," 4,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 4,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 2,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 4,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 4,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 2,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 4,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 2,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 2,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 2,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 4,\n"," 0,\n"," 0,\n"," 1,\n"," 4,\n"," 1,\n"," 1,\n"," 2,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 4,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 4,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 4,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 4,\n"," 2,\n"," 2,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 2,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 4,\n"," 2,\n"," 1,\n"," 2,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 4,\n"," 1,\n"," 0,\n"," 2,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 4,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 2,\n"," 0,\n"," 0,\n"," 2,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 4,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 2,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 2,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 2,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 2,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 2,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 4,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 2,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 4,\n"," 1,\n"," 1,\n"," 2,\n"," 0,\n"," 0,\n"," 1,\n"," 2,\n"," 2,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 2,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 2,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 2,\n"," 2,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 4,\n"," 2,\n"," 1,\n"," 1,\n"," 0,\n"," 2,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 2,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 4,\n"," 2,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 2,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 4,\n"," 1,\n"," 0,\n"," 2,\n"," 1,\n"," 2,\n"," 1,\n"," 2,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 1,\n"," 2,\n"," 2,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 2,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 4,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 1,\n"," 2,\n"," 1,\n"," 1,\n"," 0,\n"," 1,\n"," 4,\n"," 0,\n"," 2,\n"," 0,\n"," 2,\n"," 1,\n"," 0,\n"," 0,\n"," 1,\n"," 0,\n"," 1,\n"," 1,\n"," 0,\n"," 4,\n"," 1,\n"," 1,\n"," 2,\n"," 2,\n"," 0,\n"," 2,\n"," 1,\n"," 0,\n"," 0,\n"," 0,\n"," 4,\n"," 1,\n"," 1,\n"," ...]"]},"execution_count":110,"metadata":{},"output_type":"execute_result"}],"source":["pred_list"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
